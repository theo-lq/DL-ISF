{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séance 1 - Bonus : Première régularisation d'un réseau de neurones\n",
    "\n",
    "Pour poursuivre le travail du TP, on se propose d'explorer une première manière de régulariser un réseau de neurones. Nous avons vu que nous pouvions atteindre de bonne performances, mais que cela entraînait fréquemment un sur-apprentissage. \n",
    "Une des premières manières de régulariser que l'on apprend en Machine Learning est de pénaliser une régression linéaire : c'est la régression Ridge. Le principe est de modifier la fonction de perte pour contraindre les poids appris à être *petit*. Il est possible de le faire couche par couche dans un réseau de neurones. Essayons !\n",
    "\n",
    "Commençons par importer et traiter les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(style='whitegrid')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = (keras.datasets.mnist.load_data())\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, train_size=0.8)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.astype(np.float32).reshape(-1, 28 * 28)).reshape(-1, 28, 28)\n",
    "X_valid = scaler.transform(X_valid.astype(np.float32).reshape(-1, 28 * 28)).reshape(-1, 28, 28)\n",
    "X_test = scaler.transform(X_test.astype(np.float32).reshape(-1, 28 * 28)).reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pénalisation $L_2$\n",
    "\n",
    "La pénalisation à laquelle on s'intéresse ici est la pénalisation $L_2$. Si l'on considère un problème d'optimisation d'une fonction de perte $\\mathcal{L}$\n",
    "\n",
    "$$w^* = \\arg\\min_{w\\in\\mathbb{R}^d} \\mathcal{L}(w)$$\n",
    "\n",
    "Alors sa version pénalisée est la suivante avec $\\lambda > 0$ :\n",
    "\n",
    "$$w^* = \\arg\\min_{w\\in\\mathbb{R}^d} \\mathcal{L}(w) + \\lambda\\|w\\|^2$$\n",
    "\n",
    "Ce n'est plus exactement le même objectif puisqu'on force ici le vecteur solution $w^*$ a prendre de plus petite valeurs. Nous pouvons faire cela couche par couche.\n",
    "\n",
    "**Consigne** : Définir un modèle en ajoutant une pénalisation [$L_2$](https://keras.io/api/layers/regularizers/#l2-class) aux couches [`Dense`](https://keras.io/api/layers/core_layers/dense/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Lancer sur quelques époques le modèle pour valider qu'il fonctionne correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même manière que l'on souhaitais observer l'impact du learning rate sur l'entraînement, on souhaite mesurer l'apport de la régularisation au modèle. \n",
    "\n",
    "## Mesure de l'impact de la régularisation\n",
    "\n",
    "**Consigne** : Définir une fonction `get_model` qui prend en paramètre :\n",
    "* *lambda_l2* : float correspondant à la magnitude de la pénalisation $L_2$\n",
    "* *learning_rate* : float par défaut à $0.001$ correspond au learning rate de l'optimizer\n",
    "\n",
    "La fonction renvoie le modèle compilé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : En s'inspirant du travail réalisé pour le learning rate, comparer différente valeur de régularisation. Commenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
